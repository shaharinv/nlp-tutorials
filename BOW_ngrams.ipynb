{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNZ0tYv4uoh/HI9QsXJnMzJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaharinv/nlp-tutorials/blob/main/BOW_ngrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first understand how to generate n-grams using CountVectorizer\n"
      ],
      "metadata": {
        "id": "RueGymkTAX_3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQgYwePZ-l0U",
        "outputId": "374e8c6e-3aa3-4de1-cbb4-4ecb8c804939"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'thor': 5, 'hathodawala': 1, 'is': 2, 'looking': 4, 'for': 0, 'job': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "v = CountVectorizer()\n",
        "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
        "v.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = CountVectorizer(ngram_range=(1,2))\n",
        "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
        "v.vocabulary_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwqaDNN4CdIk",
        "outputId": "33a1b065-05b1-415e-bfa7-b0ee3bda5f72"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'thor': 9,\n",
              " 'hathodawala': 2,\n",
              " 'is': 4,\n",
              " 'looking': 7,\n",
              " 'for': 0,\n",
              " 'job': 6,\n",
              " 'thor hathodawala': 10,\n",
              " 'hathodawala is': 3,\n",
              " 'is looking': 5,\n",
              " 'looking for': 8,\n",
              " 'for job': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will not take a simple collection of text documents, preprocess them to remove stop words, lemmatize etc and then generate bag of 1 grams and 2 grams from it"
      ],
      "metadata": {
        "id": "sS61QkhFDXks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"Thor ate pizza\",\n",
        "    \"Loki is tall\",\n",
        "    \"Loki is eating pizza\"\n",
        "]"
      ],
      "metadata": {
        "id": "XIszqzwhDaa8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# load english language model and create nlp object from it\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def preprocess(text):\n",
        "   # remove stop words and lemmatize the text\n",
        "    doc = nlp(text)\n",
        "    filtered_tokens = []\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "          continue\n",
        "        filtered_tokens.append(token.lemma_)\n",
        "    return \" \".join(filtered_tokens)\n",
        ""
      ],
      "metadata": {
        "id": "MMZk4cz1FA-x"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(\"Thor ate pizza\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "oVLTN99nHf1j",
        "outputId": "ff9c6ef3-4604-4bbd-91af-016e81d39083"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'thor eat pizza'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(\"chinnu is naughty\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "O9AmO6euHvMU",
        "outputId": "b59be51e-9bdf-4eea-ecaa-eaa507aa7381"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'chinnu naughty'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(\"Loki is eating pizza\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "g2zmjDErIGBf",
        "outputId": "5878e1b6-869e-452f-a564-4937d7530aba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Loki eat pizza'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_preprocessed = [ preprocess(text) for text in corpus]\n",
        "\n",
        "corpus_preprocessed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7MCTyToIPj5",
        "outputId": "4f8ef63e-d69c-4b91-a41e-06d07def0f8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['thor eat pizza', 'Loki tall', 'Loki eat pizza']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = CountVectorizer(ngram_range =(1,2))\n",
        "v.fit(corpus_preprocessed)\n",
        "v.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zGpDm_wJlue",
        "outputId": "b56a9d68-7b85-433f-a995-719659c80be3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'thor': 7,\n",
              " 'eat': 0,\n",
              " 'pizza': 5,\n",
              " 'thor eat': 8,\n",
              " 'eat pizza': 1,\n",
              " 'loki': 2,\n",
              " 'tall': 6,\n",
              " 'loki tall': 4,\n",
              " 'loki eat': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now generate bag of n gram vector for few sample documents"
      ],
      "metadata": {
        "id": "PoOW9O-yMi6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v.transform([\"Thor eat pizza\"]).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn9vEGAVMUDm",
        "outputId": "404377f4-3a18-41f0-9a3e-7b5132bd6eb6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 0, 1, 0, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v.transform([\"chinnu eat pizza\"]).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYBF26AyMz-A",
        "outputId": "ba876db5-cafd-45b9-dfbc-4da457fc6d42"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 0, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nOfwDK7Ob2-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "News Category Classification Problem\n",
        "Okay now that we know basics of BAG of n grams vectorizer ðŸ˜Ž It is the time to work on a real problem. Here we want to do a news category classification. We will use bag of n-grams and traing a machine learning model that can categorize any news into one of the following categories,\n",
        "\n",
        "BUSINESS\n",
        "SPORTS\n",
        "CRIME\n",
        "SCIENCE\n"
      ],
      "metadata": {
        "id": "3ekq1nYWXZIN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MN77i3xuXbmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}